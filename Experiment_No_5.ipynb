{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzXYJB8YmG7LowX+GpI/Uj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MussaddikKhan/Data-Science-College-Practicals-/blob/main/Experiment_No_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Experiment – 5**  \n",
        "**Date:**  \n",
        "**Roll No.: 24201013**  \n",
        "**Title:** *Naive Bayes Classification Algorithm*\n",
        "\n",
        "---\n",
        "\n",
        "## **Theory**\n",
        "\n",
        "Naive Bayes is a **supervised classification algorithm** based on **Bayes’ Theorem**.  \n",
        "It predicts the class of a data point by calculating the probability of each class and selecting the one with the **highest posterior probability**.\n",
        "\n",
        "It is called *“Naive”* because the algorithm assumes that all features are **independent** of each other, even though in real-world data this is rarely true.  \n",
        "Despite this assumption, Naive Bayes performs extremely well for text-based applications such as:\n",
        "\n",
        "- Email spam detection  \n",
        "- Sentiment analysis  \n",
        "- Document classification  \n",
        "\n",
        "---\n",
        "\n",
        "## **Bayes' Theorem**\n",
        "\n",
        "<br>\n",
        "\n",
        "$$\n",
        "P(A \\mid B) = \\frac{P(B \\mid A)\\, P(A)}{P(B)}\n",
        "$$\n",
        "\n",
        "<br>\n",
        "\n",
        "Where:  \n",
        "- \\(P(A \\mid B)\\) = Posterior probability  \n",
        "- \\(P(B \\mid A)\\) = Likelihood  \n",
        "- \\(P(A)\\) = Prior probability  \n",
        "- \\(P(B)\\) = Evidence  \n",
        "\n",
        "---\n",
        "\n",
        "## **Naive Bayes Classification Formula**\n",
        "\n",
        "For a class \\(C\\) with features \\(x_1, x_2, \\dots, x_n\\):\n",
        "\n",
        "<br>\n",
        "\n",
        "$$\n",
        "P(C \\mid x_1, x_2, \\dots, x_n) =\n",
        "\\frac{P(C)\\, P(x_1 \\mid C)\\, P(x_2 \\mid C)\\dots P(x_n \\mid C)}\n",
        "{P(x_1, x_2, \\dots, x_n)}\n",
        "$$\n",
        "\n",
        "<br>\n",
        "\n",
        "Since the denominator is constant for all classes, we use:\n",
        "\n",
        "<br>\n",
        "\n",
        "$$\n",
        "P(C \\mid X) \\propto P(C)\\; \\prod_{i=1}^{n} P(x_i \\mid C)\n",
        "$$\n",
        "\n",
        "<br>\n",
        "\n",
        "Higher value → class with maximum probability → final prediction.\n",
        "\n",
        "---\n",
        "\n",
        "## **Step-wise Explanation of Naive Bayes**\n",
        "\n",
        "### **Step 1 — Calculate Prior Probability**\n",
        "Probability of each class occurring in the dataset.\n",
        "\n",
        "### **Step 2 — Calculate Likelihood**\n",
        "Probability of each feature value given each class.\n",
        "\n",
        "### **Step 3 — Apply Naive Independence**\n",
        "Multiply all likelihoods assuming features are independent.\n",
        "\n",
        "### **Step 4 — Apply Bayes’ Theorem**\n",
        "Compute the posterior for each class.\n",
        "\n",
        "### **Step 5 — Choose the Class with Highest Posterior**\n",
        "Class with maximum probability becomes the final prediction.\n",
        "\n",
        "---\n",
        "\n",
        "## **Advantages**\n",
        "- Fast and efficient  \n",
        "- Works well for text and categorical data  \n",
        "- Requires very little training data  \n",
        "- Very easy to implement  \n",
        "\n",
        "## **Disadvantages**\n",
        "- Assumes features are independent (rare in real data)  \n",
        "- Performs poorly when strong feature dependency exists  \n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "icvaFmHMW33a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "k4EAYYq-WoAO",
        "outputId": "694183d7-4d08-4ada-9964-9b12d5b172f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on training dataset: 92.86%\n",
            "Prediction for new data: No\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.naive_bayes import CategoricalNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# ---------------------------------------\n",
        "# Step 1: Create Dataset (PlayTennis)\n",
        "# ---------------------------------------\n",
        "data = [\n",
        "    [1, 1, 1, 1],\n",
        "    [1, 1, 1, 2],\n",
        "    [2, 1, 1, 1],\n",
        "    [3, 2, 1, 1],\n",
        "    [3, 3, 2, 1],\n",
        "    [3, 3, 2, 2],\n",
        "    [2, 3, 2, 2],\n",
        "    [1, 2, 1, 1],\n",
        "    [1, 3, 2, 1],\n",
        "    [3, 2, 2, 1],\n",
        "    [1, 2, 2, 2],\n",
        "    [2, 2, 1, 2],\n",
        "    [2, 1, 2, 1],\n",
        "    [3, 2, 1, 2]\n",
        "]\n",
        "\n",
        "# Labels: 1 = Yes, 0 = No\n",
        "target = [0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0]\n",
        "\n",
        "attributes = ['Outlook', 'Temperature', 'Humidity', 'Wind']\n",
        "\n",
        "# Convert to DataFrame\n",
        "data_df = pd.DataFrame(data, columns=attributes)\n",
        "target_series = pd.Series(target, dtype=\"category\")\n",
        "\n",
        "# ---------------------------------------\n",
        "# Step 2: Train Naive Bayes Classifier\n",
        "# ---------------------------------------\n",
        "model = CategoricalNB()\n",
        "model.fit(data_df, target_series)\n",
        "\n",
        "# ---------------------------------------\n",
        "# Step 3: Predict on Training Data\n",
        "# ---------------------------------------\n",
        "predicted_labels = model.predict(data_df)\n",
        "accuracy = accuracy_score(target_series, predicted_labels) * 100\n",
        "\n",
        "print(f\"Accuracy on training dataset: {accuracy:.2f}%\")\n",
        "\n",
        "# ---------------------------------------\n",
        "# Step 4: Predict a New Example\n",
        "# Example: [Outlook=1, Temperature=1, Humidity=1, Wind=2]\n",
        "# ---------------------------------------\n",
        "new_data = pd.DataFrame([[1, 1, 1, 2]], columns=attributes)\n",
        "prediction = model.predict(new_data)[0]\n",
        "\n",
        "print(\"Prediction for new data:\", \"Yes\" if prediction == 1 else \"No\")\n"
      ]
    }
  ]
}